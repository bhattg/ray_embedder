# ğŸŒ©ï¸ Scalable Ray Data Embedding Pipeline for **Qwen3-Embedding-4B**

This repository provides a **fully scalable**, **GPU-accelerated**, **Ray Dataâ€“based** embedding pipeline for generating embeddings using **Qwen3-Embedding-4B** (or any Qwen3 embedding model).

The pipeline is designed for **large-scale text embedding generation** on multi-CPU and multi-GPU nodes.

---

## ğŸš€ Features

- **End-to-end distributed pipeline** built on Ray Data  
- **Tokenizer stage (CPU)**: parallelized over all CPU cores  
- **Embedding stage (GPU)**: parallelized across all visible GPUs  
- Supports âš¡ **streaming execution**, **zero-copy Arrow batches**, and **parquet output**    
- Saves output parquet containing:
  - `prompt_id`
  - `text`
  - `embedding` (float32 list)
- Compatible with **Qwen/Qwen3-Embedding-0.6B**, **1.5B**, **4B**, etc.

---

## ğŸ§© Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSONL / Parquet Inputs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ ray.data.read_json()
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TokenizationStage (CPU)   â”‚
â”‚  â€¢ HuggingFace fast tokenizerâ”‚
â”‚  â€¢ Left padding to max_lengthâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EmbeddingStage (GPU)      â”‚
â”‚  â€¢ HF AutoModel             â”‚
â”‚  â€¢ fp16/fp32                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Save parquet (streaming)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation

```bash
pip install ray[default]
pip install transformers
pip install huggingface_hub
pip install torch
pip install pandas pyarrow
```

Make sure CUDA + PyTorch with GPU support is installed.

---

## ğŸ“ Input Format

Input must be one or many `prompt_text.jsonl` files containing:

```json
{"prompt_id": "id123", "text": "hello world"}
{"prompt_id": "id124", "text": "some text"}
```

The script automatically discovers all such files recursively.

---

## â–¶ï¸ Usage

### **Basic Run**

```bash
python embed.py   --output_dir ./embeddings/qwen3-4b/   --model_name Qwen/Qwen3-Embedding-4B   --batch_size 8   --dtype fp16
```

### **What the script will do**

- Recursively locate all `prompt_text.jsonl` files  
- Initialize Ray  
- Download/cache Qwen3 model (HuggingFace snapshot)  
- Run CPU tokenization at massive scale  
- Run GPU embedding across all GPUs  
- Save split parquet shards to `output_dir`

---

## ğŸ› ï¸ Pipeline Tuning

### CPU/GPU parallelism

```python
num_cpus = os.cpu_count()
num_gpus = torch.cuda.device_count()
```

Tokenization concurrency:

```python
concurrency=(1, 2 * num_cpus)
```

Embedding concurrency:

```python
concurrency=(1, num_gpus)
num_gpus=1
```

Tune depending on system.

---

## ğŸ§ª Output Parquet Schema

```
prompt_id: string
text: string
embedding: list<float>
```

---

## ğŸ“Š Performance Notes

- Fast tokenizer batches (size 512) â†’ **very high CPU throughput**  
- Embeddings computed in **fp16** â†’ **2â€“4Ã— GPU speedup**  
- Streaming parquet writing â†’ avoids memory blowup  
- Ray Data executor keeps memory bounded for large corpora  

---

## ğŸ”§ Environment Variables

Caches stored in:

```
/data/<user>/HF_HOME/
```

Modify:

```python
scratch = "/data/gbhatt2/HF_HOME/"
os.environ["HF_HOME"] = scratch
```

---

## â— Troubleshooting

### **Model not found**
Pre-download:

```bash
huggingface-cli download Qwen/Qwen3-Embedding-4B --local-dir ./model
```

Or run in offline mode:

```bash
export TRANSFORMERS_OFFLINE=1
```

---

## ğŸ™Œ Acknowledgements

- **Ray** team  
- **Alibaba Qwen** team  
- **HuggingFace** team  
